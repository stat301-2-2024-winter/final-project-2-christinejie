---
title: "Austin, TX: Housing Prices"
subtitle: "Data Science 2 with R (STAT 301-2)"
author: "Christine Jie"
date: today

format:
  html:
    toc: true
    toc-depth: 4
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    
execute:
  warning: false
  
from: markdown+emoji

reference-location: margin
citation-location: margin
---

::: {.callout-tip icon="false"}
## Github Repo Link

[https://github.com/stat301-2-2024-winter/final-project-2-christinejie](https://github.com/stat301-2-2024-winter/final-project-2-christinejie)
:::

```{r}
#| echo: false
load(here::here("initial_processing/price"))
load(here::here("initial_processing/house_split.rda"))
load(here::here("results/model_results.rda"))
load(here::here("results/final_results.rda"))

```




## Introduction 
I wish to predict the sales price of houses in Austin, Texas using the models we learned this quarter. I used a data set from Kaggle called [Austin, TX House Listings](https://www.kaggle.com/datasets/ericpierce/austinhousingprices). 

My motivation for creating this model is because I truly enjoy browsing real estate websites in my free time. I have always been curious about the factors that influence the price of a home, including whether it has parking, number of bedrooms, square footage, and quality of local schools. As an economics major, I'm also aware that housing prices are a large reflection of the local economy. 

The data set that I chose consists of over 15,000 home listings, including both categorical and numerical variables. 

## Data Overview
```{r}
#| echo: false
skimr::skim_without_charts(house)
```
There are no variables with any missingness. 
```{r}
#| echo: false
price_not_log
```
The `latest_price` variable my response variable. From this graph, we can see this variable is heavily right-skewed, so I decided to transform it using a log10 transformation. 

```{r}
#| echo: false
price_log
```
This is my new response variable that I will be working with for the rest of the project, `price_log10`, which is approximately normally distributed. 

## Methods

**Overview**
This is a regression and prediction problem because we are predicting house prices using a variety of variables. 

**Data Splitting**
I used 75% of the data for training the models and the remaining 25% for testing the models. 

**Model Types**

1) Baseline 

2) Boosted Tree (bt)

3) Elastic Net (en)

4) K - Nearest Neighbors (knn)

5) Simple Linear (lm)

6) Random Forest (rf)

**Tuning Parameters**
For the Boosted Tree model, I tuned the mtry, min_n and learn_rate parameters. Mtry is the number of sampled predictors, min_n is the number of data points to split, and learn_rate is weight for the influence of each new tree. 

For the Elastic Net model, I tuned the penalty and mixture parameters. Penalty is a term added during training to discourage or overfitting. Mixture controls whether the elastic model is more like a ridge model (mixture = 0) or more like a lasso model (mixture = 1). 

For the Random Forest model, I tuned the min_n and mtry parameters. min_n is the number of data points to split and mtry is the number of sampled predictors. 

For the K-Nearest Neighbors model, I tuned the neighbors parameter. neighbors is the number of nearest neighbors used when making predictions. 

**Resampling**
Resampling is when you choose random samples from the initial training dataset with replacement to estimate the population parameter repeatedly. I used v-fold cross validation with 10 folds and 5 repeats. This means the data was divided into 10 sets of equal size and we are fitting/training each model 50 times.


**Recipe Buiding** I built 4 recipes. 

1) Standard baseline/kitchen sink recipe where I used all of the predictor variables. 

2) Standard Feature Engineered recipe where I removed variables that had only a few levels which had little predictive power. I also interacted the variables for number of bathrooms and number of bedrooms to account for dependencies between these variables. (See appendix for EDA)

3) Tree based baseline recipe where I used all of the predictor variables. 

4) Tree based Feature Engineered recipe where I removed variables that had only a few levels which had little predictive power. 

**Evaluation Metric**
The metric that was used to compare models was root mean squared error, or RMSE. RMSE measures the average deviation of the predicted values from the actual values in the dataset.

## Model Building and Selection 

**Evaluation Metric**
RMSE will be used to compare the models, which measures the average deviation of the predicted values from the actual values in the dataset. 

**Table of Best Performing Model Results**
```{r}
#| echo: false
rmse_table
```
The best model is the one with the lowest RMSE. Here, it is the random forest model using the tree based feature engineered recipe. 

The worst model is the one with the highest RMSE. Here, it is the linear model using the standard feature engineered recipe. 

**Best Parameters for Each Model**
```{r}
#| echo: false
best_parameters
```
For boosted tree, the best model (with lowest RMSE) had the parameters of mtry = 7, min_n = 21, and learn_rate = 0.631. 

For KNN, the best model (with lowest RMSE) had the  parameter of neighbors = 15. 

For random forest, the best model (with lowest RMSE) had the  parameters of mtry = 7 and min_n = 2. 

For elastic net, the best model (with lowest RMSE) had the  parameters of penalty = 0.178 and mixture = 0. 

**Final Winning Model**
The winning model with the lowest RMSE was the random forest model, fitted to the feature engineered tree based recipe. The best tuning parameters were mtry = 7 and min_n = 2. This means the best random forest model had 7 data points to split and 2 sampled predictors. The lowest RMSE means this model had the lowest average deviation of the predicted values from the actual values in the dataset, which indicates the highest degree of accuracy. 

This was not surprising that this particular model and recipe won, because random forest has usually been the winning model in previous labs, and feature engineered recipes are intended to be more accurate than a kitchen sink recipe. 

## Final Model Analysis 
After fitting the winning random forest model with feature engineered recipe to the testing data, I obtained the following results. 

**Results of Final Model in Log Base 10**
```{r}
#| echo: false
house_final_metrics_log
```
These are the final results of my best model, in log base 10 transformation. 

The RMSE measures the average magnitude of difference between values predicted by a model and the actual values. Here, the RMSE is 0.114 which means the average magnitude of difference between predicted and actual log10(price) is 0.114.

RSQ represents the proportion of the variation in the dependent variable that is explained by the independent variables. Here the RSQ is 0.759 so 75.9% of the variation in log10(price) was explained by the predictor variables.

MAE represents the average absolute difference between predicted values and the actual values. Here, the MAE is 0.071, which means on average there is a 0.071 difference in predicted versus actual log10(price).

**Results of Final Model in Original Scale**

```{r}
#| echo: false
house_final_metrics_not_log
```
These are the final results of my best model, in their original scale. 

The RMSE measures the average magnitude of difference between values predicted by a model and the actual values. Here, the RMSE is 226587 which means the average magnitude of difference between predicted and actual price is $226,587. 

RSQ represents the proportion of the variation in the dependent variable that is explained by the independent variables. Here the RSQ is 0.725 so 72.5% of the variation in price was explained by the predictor variables.

MAE represents the average absolute difference between predicted values and the actual values. Here, the MAE is 91211 which means on average there is a $91,211 difference in predicted versus actual price. 

**Graphs of Predicted Prices vs Actual Prices**

```{r}
#| echo: false
knitr::include_graphics("/Users/christinejie/Desktop/stat301-2/final-project-2-christinejie/plots/plot_1.png")
```
This graph shows the relationship between predicted and actual price, for houses of all prices. For the most part, predicted price and actual price are pretty similar, as visualized by this graph. This means the model did a decent job at predicting house prices, overall. 

```{r}
#| echo: false
knitr::include_graphics("/Users/christinejie/Desktop/stat301-2/final-project-2-christinejie/plots/plot_2.png")
```
This graph shows the relationship between predicted and actual price, for houses less than $1.25M. Most of the data points are on the line of best fit. This means the model did a good job at predicting price, especially for properties that were less than around $1,250,000. 

```{r}
#| echo: false
knitr::include_graphics("/Users/christinejie/Desktop/stat301-2/final-project-2-christinejie/plots/plot_3.png")
```
This graph shows the relationship between predicted and actual price, for houses more than $1.25M. Many of the data points are not on the line of best fit. This means the model was less accurate at predicting houses with higher prices. There are a few very high outliers of actual price that are very far from the line of best fit, which can be seen on the far right side of the graph.

## Conclusion 
**Assessing and Interpreting Accuracy of Winning Model**

Although my winning model was the best out of all of the models I built and compared, the winning RMSE was still large. 

The winning model, when evaluated in its original scale, had a RMSE of 226587, which means the average magnitude of difference between predicted and actual price is $226,587. This was in part due to many of the high priced homes, which the winning model was less accurate at predicting than lower priced homes. 

I believe this is a large average magnitude of difference between predicted and actual prices. If I were a real estate agent or homebuyer using this model to evaluate house prices, an average $200,000+ discrepancy is a big deal.

However, as stated above, the model was more accurate for lower priced homes than higher priced homes. 

**Next Steps**

To improve the performance of my model, I would create two separate models: one for predicting lower priced houses, and one for predicting higher priced houses.  

I hypothesize higher priced houses are luxury houses with higher quality features that are not captured well by any of the existing predictor variables. For example, better finishes, fixtures, and the overall quality of the house are not represented in my current data set.  

I believe my hypothesis makes sense. With a quick look at Zillow listings, I can immediately see that houses with luxury features such as heated floors and granite countertops typically command a higher price per square feet and higher price per number of bedrooms, among other factors. 

**Overall Conclusion**

The model that was best performing at predicting the prices of homes in Austin, TX was the random forest model with 7 data points to split (mtry=7) and 2 sampled predictors (min_n=2), using the feature engineered recipe. This model had the lowest RMSE value of any model and recipe. 

## References 

Pierce, E. (2021). Austin, TX House Listings. Kaggle.com. https://www.kaggle.com/datasets/ericpierce/austinhousingprices

## Appendix: EDA
**Correlation Plot for All Numeric Variables**

```{r}
#| echo: false
knitr::include_graphics("/Users/christinejie/Desktop/stat301-2/final-project-2-christinejie/plots/plot_4.png")
```
This plot shows the correlation between all numeric variables. In my recipe, I decided to remove `year_built` (year built), `num_price_changes` (number of price changes), `num_of_accessibility_features` (number of accessibility features), `num_of_community_features` (number of community features). This is because these variables had close to zero correlation with the outcome variable of interest, `price_log10` (price transformed in log10). 

**Removal of Categorical Variables**

I also removed the categorical variables `city` (city), `zipcode` (zipcode), and `home_type`(home type) because these variables had 9, 48, and 10 unique values respectively. These variables had little predictive power because there were not enough unique values, so I removed them. See Data Overview section above. 

**Interaction Terms**

```{r}
#| echo: false
knitr::include_graphics("/Users/christinejie/Desktop/stat301-2/final-project-2-christinejie/plots/plot_5.png")
```
This graph shows the relationship between number of bedrooms and number of bathrooms. There is a strong positive relationship here, as number of bathrooms increases, number of bedrooms increases too. I used `step_interact()` in my recipe, with number of bathrooms and number of bedrooms as interaction terms. 

